# Project: AI Meeting Notes

## 1. Overview
A secure, local-first meeting assistant that records, transcribes, and summarizes meetings using advanced AI. 
This application prioritizes privacy by processing audio locally using Whisper, while offering powerful summarization capabilities via external LLMs (OpenRouter/Gemini).

## 2. Tech Stack

### Frontend (`app/`)
- **Runtime**: Electron
- **Framework**: React
- **Language**: TypeScript
- **Build Tool**: Vite
- **Styling**: Tailwind CSS, Chakra UI, Framer Motion
- **Icons**: React Icons, Heroicons
- **Audio Processing**: `lamejs`

### Backend (`api/`)
- **Framework**: FastAPI (Python)
- **Server**: Uvicorn
- **AI Models**:
  - **Transcription**: `openai-whisper` (Local)
  - **LLM Orchestration**: `langchain`, `langchain-openai`
- **Utilities**: `python-dotenv` for environment configuration

## 3. Architecture

### High-Level component interaction
1. **Electron App**: 
   - Manages the application lifecycle and windowing.
   - Captures microphone input and manages recording state.
   - Provides the UI for viewing transcripts and summaries.
2. **FastAPI Server**:
   - Runs as a sidecar or background process.
   - Exposes endpoints for transcription and summarization.
3. **Data Flow**:
   - **Audio**: Recorded in Frontend -> Sent to Backend -> Processed by Whisper (Local).
   - **Transcript**: Generated by Whisper -> Sent back to Frontend.
   - **Summary**: Frontend requests summary -> Backend sends transcript to OpenRouter -> Returns summary.

## 4. Project Conventions

### Directory Structure
- `app/`: Contains the Electron main process and the React renderer code.
  - `src/`: React source code (components, services, hooks).
- `api/`: Contains the Python backend code.
  - `main.py`: Entry point for the FastAPI server.
  - `services/`: (Assumed) Business logic separation.

### Development Workflow
- **Frontend**: `npm run electron-dev` (starts Vite dev server + Electron).
- **Backend**: `python main.py` (starts FastAPI on port 8000).
- **Automation**: `Makefile` exists for easy setup (`make install`, `make start-backend`, `make start-frontend`).
- **Environment**: usage of `.env` files for API keys (e.g., OpenRouter Key).

## 5. Feasibility Analysis: Real-time Whisper
**Verdict**: Feasible with current stack.

### Implementation Status (As of Jan 2026)
- **Frontend**: `RecordingView.tsx` has experimental WebSocket logic (`/ws/recording`).
- **Backend**: **Missing WebSocket implementation**. Only supports file-based transcription (`/transcribe`).
- **Plan**: Implement "Pseudo-Streaming" via WebSockets to support real-time user feedback.

### Implementation Strategy
1.  **Audio Capture**: The Electron app already captures audio.
2.  **Streaming mechanism**:
    - **Chunked Upload**: Modify `RecordingView` to send audio blobs (e.g., every 5 seconds) to `api/transcribe`.
    - **WebSockets (Better)**: Upgrade FastAPI to use `WebSocket` endpoints. Frontend streams audio binary data; Backend processes using `whisper` stream or VAD + Whisper chunks.
3.  **Performance**:
    - Local Whisper (base/small models) is fast enough on M-series Macs.
    - `faster-whisper` or `whisper.cpp` python bindings could be used for further optimization if stock `openai-whisper` is too slow.
